---
layout: post
cid: 5
title: 算法：英文分词的算法和原理
slug: 5
date: 2019/08/01 18:34:00
updated: 2021/04/13 17:14:03
status: publish
author: AntiTopQuark
categories: 
  - 算法
tags: 
  - 函数
  - int
  - 方法
  - 语言
  - 存储
  - 查询
  - 用户
  - print
  - 文件
  - 数据
  - 模型
  - 原理
  - set
  - with
  - 参数
  - 表
  - log
  - key
  - sum
  - return
  - g
  - 处理
  - 文本
  - 单词
  - 文档
  - idf
  - tf-idf
  - 公式
  - 计算
  - 特征
  - 编程
customSummary: 
noThumbInfoStyle: default
outdatedNotice: no
reprint: standard
thumb: 
thumbChoice: default
thumbDesc: 
thumbSmall: 
thumbStyle: default
---


# 英文分词的算法和原理

## 文档相关性计算公式

### TF-IDF:

**1.算法介绍**

TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。
TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。
**2.TF是词频(Term Frequency)**

词频（TF表示词条（关键字）在文本中出现的频率。这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。

​        **公式：![img](https://img-blog.csdn.net/20180807190429613?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FzaWFsZWVfYmlyZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)**           即：![请输入图片描述][1]

 其中 **ni,j** 是该词在文件 **dj** 中出现的次数，分母则是文件 dj 中所有词汇出现的次数总和；
**3. IDF是逆向文件频率(Inverse Document Frequency)**
逆向文件频率 (IDF) ：某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。
如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。
公式:![请输入图片描述][2]

其中，|D| 是语料库中的文件总数。 |{j:ti∈dj}| 表示包含词语 ti 的文件数目（即 ni,j≠0 的文件数目）。如果该词语不在语料库中，就会导致分母为零，因此一般情况下使用 1+|{j:ti∈dj}|即：
![请输入图片描述][3]

**4.TF-IDF实际上是：TF * IDF**
某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。
公式：
![请输入图片描述][4]

注：  TF-IDF算法非常容易理解，并且很容易实现，但是其简单结构并没有考虑词语的语义信息，无法处理一词多义与一义多词的情况。

**5.Python3实现**
```
# -*- coding: utf-8 -*-
from collections import defaultdict
import math
import operator
 
"""
函数说明:创建数据样本
Returns:
    dataset - 实验样本切分的词条
    classVec - 类别标签向量
"""
def loadDataSet():
    dataset = [ ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],    # 切分的词条
                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid'] ]
    classVec = [0, 1, 0, 1, 0, 1]  # 类别标签向量，1代表好，0代表不好
    return dataset, classVec
 
 
"""
函数说明：特征选择TF-IDF算法
Parameters:
     list_words:词列表
Returns:
     dict_feature_select:特征选择词字典
"""
def feature_select(list_words):
    #总词频统计
    doc_frequency=defaultdict(int)
    for word_list in list_words:
        for i in word_list:
            doc_frequency[i]+=1
 
    #计算每个词的TF值
    word_tf={}  #存储没个词的tf值
    for i in doc_frequency:
        word_tf[i]=doc_frequency[i]/sum(doc_frequency.values())
 
    #计算每个词的IDF值
    doc_num=len(list_words)
    word_idf={} #存储每个词的idf值
    word_doc=defaultdict(int) #存储包含该词的文档数
    for i in doc_frequency:
        for j in list_words:
            if i in j:
                word_doc[i]+=1
    for i in doc_frequency:
        word_idf[i]=math.log(doc_num/(word_doc[i]+1))
 
    #计算每个词的TF*IDF的值
    word_tf_idf={}
    for i in doc_frequency:
        word_tf_idf[i]=word_tf[i]*word_idf[i]
 
    # 对字典按值由大到小排序
    dict_feature_select=sorted(word_tf_idf.items(),key=operator.itemgetter(1),reverse=True)
    return dict_feature_select
 
if __name__=='__main__':
    data_list,label_list=loadDataSet() #加载数据
    features=feature_select(data_list) #所有词的TF-IDF值
    print(features)
    print(len(features))

```

### BM25:

**1.BIM(二元假设模型)**

介绍BM25模型首先要介绍二元独立模型BIM。
假设一：二元假设
    所谓二元假设，类似于布尔模型的表示方法，一篇文章在由特征表示的时候，以特征“出现”和“不出现”两种情况来表示，也可以理解为相关不相关。
假设二：词汇独立性假设
    所谓独立性假设，是指文档里出现的单词之间没有任何关联，任一个单词在文章中的分布率不依赖于另一个单词是否出现，这个假设明显与事实不符，但是为了简化计算，很多地方需要做出独立性假设，这种假设是普遍的。
    在以上两个假设的前提下，二元独立模型即可以对两个因子P(D|R)和P(D|NR)进行估算（条件概率），举个简单的例子，文档D中五个单词的出现情况如下：{1,0,1,0,1} 0表示不出现，1表示出现。用Pi表示第i个单词在相关文档中出现的概率，在已知相关文档集合的情况下，观察到文档D的概率为：
![请输入图片描述][5]

对于因子P(D|NR)，我们假设用Si表示第i个单词在在不相关文档集合中出现的概率，于是在已知不相关文档集合的情况下，观察到文档D的概率为：
![请输入图片描述][6]

于是我们可以得到下面的估算
![请输入图片描述][7]

可以将各个因子规划为两个部分，一部分是在文档D中出现的各个单词的概率乘积，另一部分是没在文档D中出现的各个单词的概率乘积，于是公式可以理解为下面的形式
![请输入图片描述][8]
对公式进行一下等价的变换，可得：
![请输入图片描述][9]

第一部分代表在文章中出现过的单词所计算得到的单词概率乘积，第二部分表示所有特征词计算得到单词概率乘积，它与具体的文档无关，所有文档该项的得分一致，所以在排序中不起作用，可以抹除掉。得到最终的估算公式：
![请输入图片描述][10]

为了方便计算，对上述公式两边取log，得到：
![请输入图片描述][11]

那么如何估算概率Si和Pi呢，如果给定用户查询，我们能确定哪些文档集合构成了相关文档集合，哪些文档构成了不相关文档集合，那么就可以用如下的数据对概率进行估算：
![请输入图片描述][12]

这个公式代表的含义就是，对于同时出现在查询Q和文档D中的单词，累加每个单词的估值结果就是文档D和查询Q的相关性度量，在预先不知道哪些文档相关哪些文档不相关的情况下，可以使用固定值代替，这种情况下该公式等价于向量空间模型（VSM）中的IDF因子，实际证明该模型的实际使用结果不好，但是它是BM25模型的基础。
**2.BM25**
BIM（二元假设模型）对于单词特征，只考虑单词是否在doc中出现过，并没有考虑单词本身的相关特征，BM25在BIM的基础上引入单词在查询中的权值，单词在doc中的权值，以及一些经验参数，所以BM25在实际应用中效果要远远好于BIM模型。

![请输入图片描述][13]

BM25由3部分组成，第一部分是BIM模型得分，上面也提到了，在一定的情况下该部分等价于IDF，第二部分是查询词在文档D中的权值，f是查询词在文档中的频率，K1和K是经验参数，第三部分是查询词自身的特征，qf是查询词在用户查询中的频率，但一般用户查询都比较短，qf一般是1，K2是经验参数，从上面的公式可以看出BM25是查询中单词的分值叠加得到，每个单词是一个个体，而整个文档被作为一个整体。
在第二部分中K因子代表了文档长度的考虑，dl是文档的长度，avdl是文档的平均长度，k1和b是调整参数，b为0时即不考虑文档长度的影响，经验表明b=0.75左右效果比较好。但是也要根据相应的场景进行调整。b越大对文档长度的惩罚越大，k1因子用于调整词频，极限情况下k1=0，则第二部分退化成1，及词频特征失效，可以证明k1越大词频的作用越大。
在我们不知道哪些文档相关，哪些文档不相关的情况下，将相关文档数R及包含查询词相关文档数r设为0，那么第一部分的BIM公式退化成：
![请输入图片描述][14]

就是IDF因子的定义，N是总文档数，n是查询词的tf信息，0.5是平滑因子。以上就是BM25的定义

## 按空格/符号分词

用正则表达式很容易
```
pattern = r'''(?x)    # set flag to allow verbose regexps
     ([A-Z]\.)+        # abbreviations, e.g. U.S.A.
   | \w+(-\w+)*        # words with optional internal hyphens
   | \$?\d+(\.\d+)?%?  # currency and percentages, e.g. $12.40, 82%
   | \.\.\.            # ellipsis
   | [][.,;"'?():-_`]  # these are separate tokens
   '''
re.findall(pattern,待分词文本)
```

## 排除Stop word

stopword就是类似 a/an/and/are/then 的这类高频词，高频词会对基于词频的算分公式产生极大的干扰，所以需要过滤

## 提取词干
词干提取( Stemming) 这是西方语言特有的处理，比如说英文单词有 单数复数的变形，-ing和-ed的变形，但是在计算相关性的时候，应该当做同一个单词。比如 apple和apples，doing和done是同一个词，提取词干的目的就是要合并这些变态

Stemming有3大主流算法

 - Porter Stemming 
 - Lovins stemmer 
 - Lancaster Stemming

Lucene 英文分词自带了3个stemming算法，分别是

- EnglishMinimalStemmer
- 著名的 Porter Stemming
- KStemmer

   https://segmentfault.com/a/1190000003839093#articleHeader8
词干提取算法并不复杂，要么是一堆规则，要么用映射表，编程容易，但是必须是这种语言的专家，了解构词法才行啊





  [1]: https://img-blog.csdn.net/20180807190512798?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FzaWFsZWVfYmlyZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70
  [2]: https://img-blog.csdn.net/20180807190920906?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FzaWFsZWVfYmlyZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70
  [3]: https://img-blog.csdn.net/20180807191126207?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FzaWFsZWVfYmlyZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70
  [4]: https://img-blog.csdn.net/201808071912424?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FzaWFsZWVfYmlyZA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70
  [5]: https://static.xuebuyuan.com/thirdpart/http://img.blog.csdn.net/20140724225032921
  [6]: https://static.xuebuyuan.com/thirdpart/http://img.blog.csdn.net/20140825153617428?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2R4aW4xMzIy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast
  [7]: https://static.xuebuyuan.com/thirdpart/http://img.blog.csdn.net/20140825154641179?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2R4aW4xMzIy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast
  [8]: https://static.xuebuyuan.com/thirdpart/http://img.blog.csdn.net/20140825155358666?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2R4aW4xMzIy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast
  [9]: https://static.xuebuyuan.com/thirdpart/http://img.blog.csdn.net/20140825155716912?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2R4aW4xMzIy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast
  [10]: https://static.xuebuyuan.com/thirdpart/http://img.blog.csdn.net/20140825172051712?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2R4aW4xMzIy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast
  [11]: https://static.xuebuyuan.com/thirdpart/http://img.blog.csdn.net/20140825172105000?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2R4aW4xMzIy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast
  [12]: https://static.xuebuyuan.com/thirdpart/http://img.blog.csdn.net/20140825173200750?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2R4aW4xMzIy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast
  [13]: https://static.xuebuyuan.com/thirdpart/http://img.blog.csdn.net/20140825180941890?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2R4aW4xMzIy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast
  [14]: https://static.xuebuyuan.com/thirdpart/http://img.blog.csdn.net/20140825184857968?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2R4aW4xMzIy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast