---
layout: post
cid: 69
title: Spark 源码阅读系列
slug: 69
date: 2021/04/13 20:28:47
updated: 2021/04/13 20:28:47
status: publish
author: AntiTopQuark
categories: 
  - Spark
  - 大数据
tags: 
  - rdd
  - 节点
  - 逻辑
  - 实例
  - 方法
  - 内存
  - 存储
  - 进程
  - 文件
  - spark
  - 分区
  - map
  - 数据
  - task
  - 记录
  - rpc
  - 机器
  - 流程
  - 启动
  - sbin
  - 过程
  - master
  - worker
  - ala
  - akka
  - actor
  - netty
  - 索引
  - apache
  - 参数
  - key
  - web
  - scala
  - val
  - 计算
  - reduce
  - partition
  - 问题
  - hash
  - hashmap
  - 磁盘
customSummary: 
noThumbInfoStyle: default
outdatedNotice: no
reprint: standard
thumb: 
thumbChoice: default
thumbDesc: 
thumbSmall: 
thumbStyle: default
---



# 1. 集群启动流程
## 启动过程 sbin/start-all.sh
1. 加载Spark配置信息，运行`${SPARK_HOME}/sbin/spark-config.sh`
2. 在当前机器上，启动Master节点，运行`${SPARK_HOME}/sbin"/start-master.sh`,在start-master.sh中使用spark-daemon.sh加载CLASS="org.apache.spark.deploy.master.Master"
3. 启动slaves中的work节点，运行sbin/start-worker.sh,加载CLASS="org.apache.spark.deploy.worker.Worker"

## Master类 core\src\main\scala\org\apache\spark\deploy\master\Master.scala

master进程的开启就是master类实例化的过程，Master类继承实现了ThreadSafeRpcEndpoint特质(core\src\main\scala\org\apache\spark\rpc\RpcEndpoint.scala)

Spark使用netty作为通信机制实现rpc，很多书都说spark的通信机制使用的是actor或者Akka，其实现在已经actor和akka都不用了。在最开始的spark中，通讯机制是actor，Master类继承的就是是actor类，然后通信机制换成了Akka，一直到spark1.6，废弃了actor，从spark1.6开始，通信机制是akka与netty共存，但当spark2.0版本之后，akka也被放弃，使用netty传输消息，这是因为akka的版本问题，如使用akka发送消息，就要求发送端和接收端的akka版本相同，不够灵活，而且spark使用的akka特性很少，不需要akka发布的新版本，而且很容易实现，所以就在2.0版本彻底移除了akka，换成了netty

Master负责监视接收心跳，有checkTimeout()方法，检测心跳是否超时，WORKER_TIMEOUT定义的默认是60s。

## Worker类 core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala

worker进程的开启就是worker类实例化的过程。注意的是worker有个register方法（registerWithMaster），也就是向master节点注册worker节点，master也有一个receive方法接收它，注册成功后master会回复worker节点，发送RegisteredWorker消息，这个消息就是一个master的url和一个master的web ui的url，然后就是发送心跳了，默认是15s发送一次（代码中是WORKER_TIMEOUT/4）
# 3. cache和persist

存储块block和partition关系:
rdd的运算是基于分区partition的，partition是逻辑上的概念，block是物理上的数据实体，一个rdd的partition就对应一个storage模块的block
DiskStore磁盘存储和MemoryStore内存存储:
DiskStore磁盘存储：
spark会在磁盘上创建spark文件夹，命名为（spark-local-x年x月x日时分秒-随机数）
block块都会存在这里，然后把block id映射成相应的文件路径，就可以存取文件了
MemoryStore内存存储：
更简单，使用hashmap管理block就行了，block id作为key，MemoryEntry为value
https://blog.csdn.net/dkl12/article/details/80742498/
# 5. shuffle
![71332-u76i9pr2prh.png](http://www.sukidesu.top/usr/uploads/2020/10/2044081415.png)
spark的shuffle过程：Spark shuffle分为write和read两个过程
1. shuffle write:将MapTask产生的数据写到磁盘中，首先将map的结果文件中的数据记录送到对应的bucket里面（缓冲区），之后，每个bucket里面的数据会不断被写到本地磁盘上，形成一个ShuffleBlockFile，或者简称FileSegment
2. shuffle read:ReduceTask再将数据抓取（fetch）过来,因为不需要排序，fetch一旦开始，就会边fetch边处理（reduce）

## HashShuffle:
HashShuffle是最开始存在Shuffle机制，在2.0版本以后就弃用了，下面我们了解一下。我们要知道有多少个reduce task就有多少个buffer
1）普通机制：
![08398-7xwkmrbje0n.png](http://www.sukidesu.top/usr/uploads/2020/10/4048825525.png)
执行流程：
1. 每一个map task将不同结果写到不同的buffer中，每个buffer的大小为32K。buffer起到数据缓存的作用。
2. 每个buffer文件最后对应一个磁盘小文件。
3. reduce task来拉取对应的磁盘小文件。

存在问题：
1. 小文件过多，耗时低效的IO操作 
2. OOM，读写文件以及缓存过多

2）进行优化：
![67575-mcfmff7uxu.png](http://www.sukidesu.top/usr/uploads/2020/10/2301294282.png)
每个core中的所有task对应一个Buffler内存，这样就减少了落地小文件和IO开销，不过当reduce tast有好多的时候，显然进行优化的方式也不好，于是延伸出了sortBuffle

## SortShuffle:
SortShuffle的运行机制主要分成两种： 普通运行机制       bypass运行机制
1）普通运行机制：
![61583-dzwnn79kg6.png](http://www.sukidesu.top/usr/uploads/2020/10/3454487654.png)
执行流程:
1. map task 的计算结果会写入到一个内存数据结构里面，内存数据结构默认是5M
2. 在shuffle的时候会有一个定时器，不定期的去估算这个内存结构的大小，当内存结构中的数据超过5M时,比如现在内存结构中的数据为5.01M，那么他会申请5.01*2-5=5.02M内存给内存数据结构。
3. 如果申请成功不会进行溢写，如果申请不成功，这时候会发生溢写磁盘。
4. 在溢写之前内存结构中的数据会进行排序分区
5. 然后开始溢写磁盘，写磁盘是以batch的形式去写，一个batch是1万条数据，
6. map task执行完成后，会将这些磁盘小文件合并成一个大的磁盘文件，同时生成一个索引文件。
7. reduce task去map端拉取数据的时候，首先解析索引文件，根据索引文件再去拉取对应的数据。

2）bypass机制
![95168-lbqkt35rdx.png](http://www.sukidesu.top/usr/uploads/2020/10/608103351.png)
bypass机制运行条件：
- shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。
- 不是聚合类的shuffle算子（比如reducebykey算子用的就是普通机制shuffle，sortbykey算子、repartition算子用的就是bypass机制）。

在这种机制下，当前stage的task会为每个下游的task都创建临时磁盘文件。将数据按照key值进行hash，然后根据hash值，将key写入对应的磁盘文件中（个人觉得这也相当于一次另类的排序，将相同的key放在一起了）。最终，同样会将所有临时文件依次合并成一个磁盘文件，建立索引。
该机制与未优化的hashshuffle相比，没有那么多磁盘文件，下游task的read操作相对性能会更好。
该机制与sortshuffle的普通机制相比，在readtask不多的情况下，首先写的机制是不同，其次不会进行排序。这样就可以节约一部分性能开销。

