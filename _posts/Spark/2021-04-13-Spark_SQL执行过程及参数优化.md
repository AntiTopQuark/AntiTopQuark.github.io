---
layout: post
cid: 66
title: Spark SQL执行过程及参数优化
slug: 66
date: 2021/04/13 20:28:39
updated: 2021/04/13 20:28:39
status: publish
author: AntiTopQuark
categories: 
  - Spark
tags: 
  - 类型
  - 节点
  - sql
  - 逻辑
  - 方法
  - 语句
  - 存储
  - 状态
  - 文件
  - hdfs
  - spark
  - 分区
  - 数据
  - 操作
  - join
  - 机器
  - 过程
  - 原理
  - 架构
  - user
  - 参数
  - 表
  - 叶子
  - select
  - 区别
  - struct
  - 环境
  - 处理
  - 计算
  - partition
  - 步骤
  - 问题
  - hash
  - p_date
customSummary: 
noThumbInfoStyle: default
outdatedNotice: no
reprint: standard
thumb: 
thumbChoice: default
thumbDesc: 
thumbSmall: 
thumbStyle: default
---


# Spark SQL原理和架构
## 原理
![92752-36yjsb5ibi6.png](http://www.sukidesu.top/usr/uploads/2020/10/3373336880.png)
1. 当Spark读取文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。
2. 随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。
3. 随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。
![05368-7elnrco9gi8.png](http://www.sukidesu.top/usr/uploads/2020/10/318429011.png)
- 每个节点可以起一个或多个Executor。
- 每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。
- 每个Task执行的结果就是生成了目标RDD的一个partiton。
注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。
而 Task被执行的并发度 = Executor数目 * 每个Executor核数。
## 架构
Spark SQL 的整体架构如下图所示
![54070-24ftlw4ndvg.png](http://www.sukidesu.top/usr/uploads/2020/10/2642010341.png)
从上图可见，无论是直接使用 SQL 语句还是使用 DataFrame，都会经过如下步骤转换成 DAG 对 RDD 的操作
- Spark SQL Parser 解析 SQL，生成 Unresolved Logical Plan(未解析的逻辑算子树)。Spark使用ANTLR作为SQL解析器，有兴趣的同学可以了解下:[ANTLR4简易入门][1]
- 由 Analyzer 结合 Catalog 信息生成 Resolved Logical Plan(解析后的逻辑算子树),此时对算子树的节点绑定了各种数据信息，包括每张表对应的字段集，字段类型，数据存储位置等等
- Optimizer根据预先定义好的规则，例如谓词、算子下推，对 Resolved Logical Plan 进行优化并生成 Optimized Logical Plan(优化后的逻辑算子树),这种优化规则属于RBO(Rule-based optimization)
- Query Planner 将 Optimized Logical Plan 转换成多个 Physical Plan
- CBO(Cost-based optimization) 根据 Cost Model 算出每个 Physical Plan 的代价并选取代价最小的 Physical Plan 作为最终的 Physical Plan
- Spark 以 DAG 的方法执行上述 Physical Plan
- 在执行 DAG 的过程中，Adaptive Execution 根据运行时信息动态调整执行计划从而提高执行效率
 
## 物理计划介绍
可以在Spark Web UI的DAG图下方的Datail中看到执行计划。或者可以使用 `explain` 来查看SQL的物理计划，或者使用 `explain extended ` 来查看Spark SQL的所有执行计划。例如：

``` 
explain extended  
 select
     ad_id,
     p_date
 from
     ad_dim.dim_ad
 where
     p_date = '20200801'
 limit
     20
```

或者可以在Shell里，使用 `spark.sql(' ......').explain(True) `
它的四种执行计划为：
```
== Parsed Logical Plan ==
'GlobalLimit 20
+- 'LocalLimit 20
   +- 'Project ['ad_id, 'p_date]
      +- 'Filter ('p_date = 20200801)
         +- 'UnresolvedRelation `ad_dim`.`dim_ad`

== Analyzed Logical Plan ==
ad_id: bigint, p_date: string
GlobalLimit 20
+- LocalLimit 20
   +- Project [ad_id#2492L, p_date#2574]
      +- Filter (p_date#2574 = 20200801)
         +- SubqueryAlias dim_ad
            +- Relation[ad_id#2492L,ad_name#2493,inventory_types#2494,advertiser_id#2495L,campaign_id#2496L,pricing_type#2497,create_time#2498,modify_time#2499,ad_status#2500,audit_status#2501,opt_status#2502,isdel#2503,opt_time#2504,operator_type#2505,ad_version#2506,site_id#2507,content_owner_type#2508,convert_id#2509L,cpa_bid#2510L,cpa_phrase#2511,cpa_auto#2512,cpa_bid_type#2513,cpa_bid_ratio#2514,cpa_skip_first_phrase#2515,... 59 more fields] orc

== Optimized Logical Plan ==
GlobalLimit 20
+- LocalLimit 20
   +- Project [ad_id#2492L, p_date#2574]
      +- Filter (isnotnull(p_date#2574) && (p_date#2574 = 20200801))
         +- Relation[ad_id#2492L,ad_name#2493,inventory_types#2494,advertiser_id#2495L,campaign_id#2496L,pricing_type#2497,create_time#2498,modify_time#2499,ad_status#2500,audit_status#2501,opt_status#2502,isdel#2503,opt_time#2504,operator_type#2505,ad_version#2506,site_id#2507,content_owner_type#2508,convert_id#2509L,cpa_bid#2510L,cpa_phrase#2511,cpa_auto#2512,cpa_bid_type#2513,cpa_bid_ratio#2514,cpa_skip_first_phrase#2515,... 59 more fields] orc

== Physical Plan ==
*CollectLimit 20
+- *LocalLimit 20
   +- *Project [ad_id#2492L, p_date#2574]
      +- *FileScan orc ad_dim.dim_ad[ad_id#2492L,p_date#2574] Batched: true, Format: ORC, Location: PrunedInMemoryFileIndex[hdfs://haruna/user/tiger/warehouse/ad_dim.db/dim_ad/p_date=20200801], PartitionCount: 1, PartitionFilters: [isnotnull(p_date#2574), (p_date#2574 = 20200801)], PushedFilters: [], ReadSchema: struct<ad_id:bigint>
[ ]:
```
## 算子简介
物理计划的节点分为三种：
1. LeafExecNode类型。叶子节点类型的物理计划不存在子节点，一般与数据源相关的节点都属于该类型
2. UnaryExecNode类型,这种节点是一元的。比如Exchange、Project等等，主要是用来对RDD进行transform操作
3. BinaryExecNode类型，这种节点是二元的，主要是Join执行计划，例如BoradcastHashJoinExec、SortMergeJoinExec、ShuffleHashJoinExec等等。

简单介绍几个算子：
1. 数据文件扫描算子 FileSourceScanExec
作为物理计划的叶子节点，FileSourceScanExec中的分区排序信息会根据数据文件的初始RDD进行设置。如果没有bucket信息，则分区与排序操作将分别为最简单的UnknownPartitioning和Nil.
UnknownPartitioning其实就是不进行分区。
2. 过滤执行算子 FilterExec && 列裁剪执行算子 ProjectExec  这两个算子在执行时，会仍沿用子节点的方法，不会对RDD分区进行任何新操作。
3. EnsureRequirements规则 
在特定情形下，SparkPlan 对输入数据的分布（ Distribution ）情况和排序（Ordering）特性有着一定的要求。例如，SortMerge 类型的 Join 算子，要求输入数据已经按照 Hash 方式分区且处于有序状态。如果输入数据的分布或有序性无法满足当前节点的处理逻辑，则 EnsureRequirements 规则会在物理计划中添加一些 Shuffle 操作或排序操作来达到要求，体现在物理算子树上就是加入 Exchange 节点或 SortExec 节点。
    a. 添加Exchange节点：Exchange 本身也是 UnaryExecNode 类型的 SparkPlan ，在 Spark SQL 中被定义为抽象类。继承 Exchange 的子类有 BroadcastExchangeExec 和 ShuffleExchange 两种。很明显，ShuffleExchange 会通过 Shuffle 操作进行重分区处理，而BroadcastExchangeExec 则对应广播操作。Exchange 节点是实现数据并行化的重要算子，用于解决数据分布（ Distribution ）相关问题。具体来讲，需要添加 Exchange 节点的情形有以下两种。
        - 数据分布不满足：子节点的物理计划输出数据无法满足（Satisfies）当前物理计划处理逻辑中对数据分布的要求，例如子节点输出数据分布为 UnspecifiedDistribution ，而当前物理计划对输入数据分布的需求是 OrderedDistribution 。
        - 数据分布不兼容：当前物理计划为 BinaryExecNode 类型，即存在两个子物理计划时，两个子物理计划的输出数据可能不兼容（Compatile）。例如，Hash 的方式不同，导致应该在同一个分区的数据最终落到不同的节点上。在这种情况下，也需要创建 Exchange 节点重新进行 Shuffle 操作。
    b. 应用 ExchangeCoordinator 协调分区:ExchangeCoordinator顾名思义，是Exchange协调器，是一个用于决定怎么在stage之间进行shuffle数据的coordinator。这个协调器用于决定之后的shuffle有多少个partition用来需要fetch shuffle 数据。实际上，这个需求在 Spark SQL 生产环境中是非常迫切的，Shuffle 后的分区数目如果设置得太大，则会相应地分配计算单元，带来调度压力且导致资源浪费；如果设置得太小，则单个分区数据量大，处理起来性能低，甚至导致 OOM 等问题出现。作为公共的平台，能够根据数据特点自动计算合理的分区数目是一个必不可少的能力，ShuffleExchange 中默认的固定设置显然无法满足该需求。
    ExchangeCoordinator 是 ShuffleExchange 的 Option 类型构造参数，默认情况下不会被创建。如果要使用，需要满足两个条件。
        - Spark SQL 的自适应机制开启，对应的参数（ spark.sql.adaptive.enabled ）设置为 true。
        - 这批 SparkPlan 节点能够支持协调器，一种情况是至少存在一个 ShuffleExchange 类型的节点且所有节点的输出分区方式都是 HashPartitioning ，另一种情况是节点数目大于 1 且每个节点输出数据的分布都是 ClusteredDistribution 类型。
4. Join算子：
对于Spark来说有3种Join的实现，每种Join对应着不同的应用场景：
    - Broadcast Hash Join ： 适合一张较小的表和一张大表进行join
    - Shuffle Hash Join :  适合一张小表和一张大表进行join，或者是两张小表之间的join
    - Sort Merge Join ： 适合两张较大的表之间进行join
前两者都基于的是Hash Join，只不过在hash join之前需要先shuffle还是先broadcast。

# 其他：
Spark join实现原理 https://www.cnblogs.com/JP6907/p/10721436.html
Spark shuffle 原理:https://blog.csdn.net/weixin_43806056/article/details/103565412?utm_medium=distribute.pc_relevant.none-task-blog-title-3&spm=1001.2101.3001.4242
Spark 数据倾斜： https://blog.csdn.net/kaede1209/article/details/81145560
Spark groupBykey 和 reduceBykey的区别：http://www.mamicode.com/info-detail-2171306.html
[Spark SQL执行过程及参数优化 副本.docx][2]


  [1]: https://zhuanlan.zhihu.com/p/114982293
  [2]: http://www.sukidesu.top/usr/uploads/2020/10/2853132573.docx
  
  
